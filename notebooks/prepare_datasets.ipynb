{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import pathlib\n",
    "from typing import Any, Callable, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "from torchvision.datasets import VisionDataset\n",
    "from torchvision.datasets.utils import verify_str_arg, check_integrity\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/fer2013/full.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>pixels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emotion                                             pixels\n",
       "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5742, 2), (22967, 2))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.shape, train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "val.to_csv(\"../data/fer2013/val.csv\", index=False)\n",
    "train.to_csv(\"../data/fer2013/train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FER2013(VisionDataset):\n",
    "    \"\"\"`FER2013\n",
    "    <https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge>`_ Dataset.\n",
    "\n",
    "    Args:\n",
    "        root (string): Root directory of dataset where directory\n",
    "            ``root/fer2013`` exists.\n",
    "        split (string, optional): The dataset split, supports ``\"train\"`` (default), or ``\"test\"``.\n",
    "        transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed\n",
    "            version. E.g, ``transforms.RandomCrop``\n",
    "        target_transform (callable, optional): A function/transform that takes in the target and transforms it.\n",
    "    \"\"\"\n",
    "\n",
    "    _RESOURCES = {\n",
    "        \"train\": (\"train.csv\", None),\n",
    "        \"test\": (\"test.csv\", None),\n",
    "        \"val\": (\"val.csv\", None),\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str,\n",
    "        split: str = \"train\",\n",
    "        transform: Optional[Callable] = None,\n",
    "        target_transform: Optional[Callable] = None,\n",
    "    ) -> None:\n",
    "        self._split = verify_str_arg(split, \"split\", self._RESOURCES.keys())\n",
    "        super().__init__(root, transform=transform, target_transform=target_transform)\n",
    "\n",
    "        base_folder = pathlib.Path(self.root) / \"fer2013\"\n",
    "        file_name, md5 = self._RESOURCES[self._split]\n",
    "        data_file = base_folder / file_name\n",
    "        if not check_integrity(str(data_file), md5=md5):\n",
    "            raise RuntimeError(\n",
    "                f\"{file_name} not found in {base_folder} or corrupted. \"\n",
    "                f\"You can download it from \"\n",
    "                f\"https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge\"\n",
    "            )\n",
    "\n",
    "        with open(data_file, \"r\", newline=\"\") as file:\n",
    "            self._samples = [\n",
    "                (\n",
    "                    torch.tensor(\n",
    "                        [int(idx) for idx in row[\"pixels\"].split()], dtype=torch.uint8\n",
    "                    ).reshape(48, 48),\n",
    "                    int(row[\"emotion\"]) if \"emotion\" in row else None,\n",
    "                )\n",
    "                for row in csv.DictReader(file)\n",
    "            ]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._samples)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[Any, Any]:\n",
    "        image_tensor, target = self._samples[idx]\n",
    "        image = Image.fromarray(image_tensor.numpy())\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"split={self._split}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FER2013(\"../data\", \"train\")\n",
    "valid_dataset = FER2013(\"../data\", \"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"../data/fer2013/train\"\n",
    "val_dir = \"../data/fer2013/val\"\n",
    "emo_map = {\n",
    "    0: \"angry\",\n",
    "    1: \"disgust\", \n",
    "    2: \"fear\", \n",
    "    3: \"happy\", \n",
    "    4: \"sad\",\n",
    "    5: \"surprise\", \n",
    "    6: \"neutral\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22967/22967 [00:04<00:00, 4834.99it/s]\n"
     ]
    }
   ],
   "source": [
    "for i, (image, target) in enumerate(tqdm(train_dataset)):\n",
    "    folder = emo_map[target]\n",
    "    os.makedirs(os.path.join(train_dir, folder), exist_ok=True)\n",
    "    image.save(os.path.join(train_dir, folder, str(i) + \".png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5742/5742 [00:01<00:00, 4842.43it/s]\n"
     ]
    }
   ],
   "source": [
    "for i, (image, target) in enumerate(tqdm(valid_dataset)):\n",
    "    folder = emo_map[target]\n",
    "    os.makedirs(os.path.join(val_dir, folder), exist_ok=True)\n",
    "    image.save(os.path.join(val_dir, folder, str(i) + \".png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 225.99it/s]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "ck_p = Path(\"../data/CK+48\")\n",
    "fer_p = Path(\"../data/fer2013/\")\n",
    "ck2fer = {\n",
    "    \"anger\": \"angry\",\n",
    "    \"contempt\": \"contempt\",\n",
    "    \"disgust\": \"disgust\",\n",
    "    \"fear\": \"fear\",\n",
    "    \"happy\": \"happy\",\n",
    "    \"sadness\": \"sad\",\n",
    "    \"surprise\": \"surprise\"\n",
    "}\n",
    "for e in tqdm(list(ck_p.iterdir())):\n",
    "    images_paths = list(e.iterdir())\n",
    "    train_imgs, val_imgs = train_test_split(images_paths, test_size=0.2, random_state=42)\n",
    "    (fer_p / \"train\" / ck2fer[e.name]).mkdir(exist_ok=True)\n",
    "    (fer_p / \"val\" / ck2fer[e.name]).mkdir(exist_ok=True)\n",
    "    for ti in train_imgs:\n",
    "        ti.rename(fer_p / \"train\" / ck2fer[e.name] / ti.name)\n",
    "    for vi in val_imgs:\n",
    "        vi.rename(fer_p / \"val\" / ck2fer[e.name] / vi.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['angry', 0.8832],\n",
       " ['contempt', 0.9985],\n",
       " ['disgust', 0.9825],\n",
       " ['fear', 0.8841],\n",
       " ['happy', 0.7942],\n",
       " ['neutral', 0.8614],\n",
       " ['sad', 0.8639],\n",
       " ['surprise', 0.9049]]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = []\n",
    "for e in list((fer_p / \"train\").iterdir()):\n",
    "    weights.append([e.name, len(list(e.iterdir()))])\n",
    "\n",
    "def f(w):\n",
    "    new_w = list(w)\n",
    "    new_w[1] = round(1 - w[1] / len(df), 4)\n",
    "    return new_w\n",
    "\n",
    "weights = list(map(lambda x: f(x), weights))\n",
    "sorted(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "ck_p = Path(\"../data/CK+48\")\n",
    "final_p = Path(\"../data/ck_data\")\n",
    "\n",
    "for e in ck_p.iterdir():\n",
    "    images_paths = list(e.iterdir())\n",
    "    train_imgs, val_imgs = train_test_split(images_paths, test_size=0.2, random_state=42)\n",
    "    (final_p / \"train\" / e.name).mkdir(exist_ok=True)\n",
    "    (final_p / \"val\" / e.name).mkdir(exist_ok=True)\n",
    "    for ti in train_imgs:\n",
    "        ti.rename(final_p / \"train\" / e.name / ti.name)\n",
    "    for vi in val_imgs:\n",
    "        vi.rename(final_p / \"val\" / e.name / vi.name)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install \n",
    "- https://www.kaggle.com/datasets/uwrfkaggler/ravdess-emotional-speech-audio\n",
    "- https://www.kaggle.com/datasets/ejlok1/surrey-audiovisual-expressed-emotion-savee\n",
    "- https://www.kaggle.com/datasets/ejlok1/toronto-emotional-speech-set-tess\n",
    "\n",
    "**RAVDESS**\n",
    "- Modality (01 = full-AV, 02 = video-only, 03 = audio-only).\n",
    "- Vocal channel (01 = speech, 02 = song).\n",
    "- Emotion **(01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised)**.\n",
    "- Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion.\n",
    "- Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\").\n",
    "- Repetition (01 = 1st repetition, 02 = 2nd repetition).\n",
    "- Actor (01 to 24. Odd numbered actors are male, even numbered actors are female).\n",
    "\n",
    "\n",
    "\n",
    "**SAVEE**\n",
    "- four native English male speakers (identified as DC, JE, JK, KL),\n",
    "- Emotion has been described psychologically in discrete categories: **anger, disgust, fear, happiness, sadness, surprise, neutral**. \n",
    "\n",
    "**TESS**\n",
    "- There are a set of 200 target words were spoken in the carrier phrase \"Say the word _' by two actresses (aged 26 and 64 years) and recordings were made of the set portraying each of seven emotions **(anger, disgust, fear, happiness, pleasant surprise, sadness, and neutral)**. There are 2800 data points (audio files) in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_DIR = Path(\"../data/emotions_audio/\")\n",
    "RAVDESS_DIR = Path(\"../data/emotions_audio_todo/RAVDESS\")\n",
    "SAVEE_DIR = Path(\"../data/emotions_audio_todo/SAVEE\")\n",
    "TESS_DIR = Path(\"../data/emotions_audio_todo/TESS\")\n",
    "\n",
    "ravdes_map = {\n",
    "    \"01\": \"neutral\",\n",
    "    \"02\": \"neutral\",\n",
    "    \"03\": \"happy\",\n",
    "    \"04\": \"sad\",\n",
    "    \"05\": \"angry\",\n",
    "    \"06\": \"fear\",\n",
    "    \"07\": \"disgust\",\n",
    "    \"08\": \"surprise\"\n",
    "}\n",
    "savee_map = {\n",
    "    \"a\": \"angry\",\n",
    "    \"d\": \"disgust\",\n",
    "    \"f\": \"fear\",\n",
    "    \"h\": \"happy\",\n",
    "    \"n\": \"neutral\",\n",
    "    \"sa\": \"sad\",\n",
    "    \"su\": \"surprise\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1152/1152 [00:00<00:00, 1345.38it/s]\n",
      "100%|██████████| 288/288 [00:00<00:00, 1339.64it/s]\n"
     ]
    }
   ],
   "source": [
    "def prepare_ravdess():\n",
    "    wavs = list(RAVDESS_DIR.rglob(\"*.wav\"))\n",
    "    train_wavs, val_wavs = train_test_split(wavs, test_size=0.2, random_state=42)\n",
    "    for stage, wavs in zip([\"train\", \"val\"], [train_wavs, val_wavs]):\n",
    "        for wav in tqdm(wavs):\n",
    "            code = wav.name.split(\"-\")[2]\n",
    "            e = ravdes_map[code]\n",
    "            (FINAL_DIR / stage / e).mkdir(exist_ok=True)\n",
    "            shutil.copy(wav, FINAL_DIR / stage / e / wav.name)\n",
    "prepare_ravdess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 384/384 [00:00<00:00, 3120.40it/s]\n",
      "100%|██████████| 96/96 [00:00<00:00, 3576.91it/s]\n"
     ]
    }
   ],
   "source": [
    "def prepare_savee():\n",
    "    wavs = list(SAVEE_DIR.rglob(\"*.wav\"))\n",
    "    train_wavs, val_wavs = train_test_split(wavs, test_size=0.2, random_state=42)\n",
    "    for stage, wavs in zip([\"train\", \"val\"], [train_wavs, val_wavs]):\n",
    "        for wav in tqdm(wavs):\n",
    "            code = wav.name[3]\n",
    "            if code == \"s\":\n",
    "                code += wav.name[4]\n",
    "            e = savee_map[code]\n",
    "            (FINAL_DIR / stage / e).mkdir(exist_ok=True)        \n",
    "            shutil.copy(wav, FINAL_DIR / stage / e / wav.name)\n",
    "prepare_savee()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2240/2240 [00:00<00:00, 5487.24it/s]\n",
      "100%|██████████| 560/560 [00:00<00:00, 6057.54it/s]\n"
     ]
    }
   ],
   "source": [
    "def prepare_tess():\n",
    "    wavs = list(TESS_DIR.rglob(\"*.wav\"))\n",
    "    train_wavs, val_wavs = train_test_split(wavs, test_size=0.2, random_state=42)\n",
    "    for stage, wavs in zip([\"train\", \"val\"], [train_wavs, val_wavs]):\n",
    "        for wav in tqdm(wavs):\n",
    "            e = wav.parts[-2].split(\"_\")[-1]\n",
    "            (FINAL_DIR / stage / e).mkdir(exist_ok=True)        \n",
    "            shutil.copy(wav, FINAL_DIR / stage / e / wav.name)\n",
    "prepare_tess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a855c4efa1bcffec0e0592d87e36d2dc47c870182d52bf8f025634bf46aad6f3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('emo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
